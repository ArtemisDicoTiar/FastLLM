import torch
from torch import Tensor
from torch.nn import Module, functional as F
from typing import Tuple
from transformers import T5ForConditionalGeneration

# Helpers function

# copy from https://github.com/LeeSinLiang/microGPT/blob/ed40cf9780dbeb180adfe94c227d4aa97e69250e/gpt.py
def top_k_top_p_filter(logits:Tensor, top_k:int = 0, top_p:float = 0.0) -> Tensor:
    """
    Filter a distribution of logits using top-k and top-p filtering.
    :param logits: logits distribution.
    :param top_k: keep top k tokens with highest probability.
    :param top_p: keep top tokens with cumulative probability >= top_p.
    :return: logits distribution.
    """
    if top_k > 0:
        filter = torch.topk(logits, min(top_k, logits.size(-1)))[0]
        logits[logits < filter[:, [-1]]] = float('-inf')
    if top_p > 0.0:
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        filter = cumulative_probs > top_p
        filter[..., 1:] = filter[..., :-1].clone()
        filter[..., 0] = 0
        indices_to_remove = filter.scatter(1, sorted_indices, filter)
        logits[indices_to_remove] = float('-inf')
    return logits


def norm_logits(logits:Tensor, temperature:float, top_k:float, top_p:float) -> torch.Tensor:
    """
    Normalize logits.
    :param logits: logits distribution.
    :param temperature: temperature.
    :param top_k: keep top k tokens with highest probability.
    :param top_p: keep top tokens with cumulative probability >= top_p.
    :return: normalized logits distribution.
    """
    logits = logits / temperature
    logits = top_k_top_p_filter(logits, top_k=top_k, top_p=top_p)
    probs = F.softmax(logits, dim=0)
    return probs


def sample(logits:Tensor, num_samples:int = 1):
    """
    Sample from a distribution.
    :param logits: logits distribution.
    :param num_samples: number of samples.
    :return: sampled tokens.
    """
    idx_next = torch.multinomial(logits, num_samples=num_samples)
    if (idx_next.item() == 0):
        raise RuntimeError
    return idx_next


def max_fn(x:Tensor) -> Tensor:
    """
    Max function.
    :param x: input tensor.
    :return: tensor norm(max(0, x)).
    """
    x_max = torch.where(x > 0, x, torch.zeros_like(x))
    x_max_sum = torch.sum(x_max, dim=0, keepdim=True) 
    return x_max / x_max_sum


@torch.no_grad()
def speculative_decoding(input_ids:Tensor, drafter:Module, target:Module, max_len:int, gamma:int = 4,
                         temperature:float = 1, top_k:int = 0, top_p:float = 0) -> Tuple[Tensor, float]:
    """
    Implementation of Speculative Decoding. (https://arxiv.org/pdf/2211.17192.pdf)

    :param input_ids: input/prefix sequence.
    :param drafter: drafter model.
    :param target: target model.
    :param max_len: maximum length of the output sequence.
    :param gamma: gamma, number of drafts generated by the drafter.
    :param temperature: temperature for sampling.
    :param top_k: top_k for sampling.
    :param top_p: top_p for sampling.
    :return: generated sequence, accept rate.
    """

    seq = input_ids
    seq_len = seq.shape[1]
    maximum_length = seq_len + max_len
    total_accept = 0
    number_of_trials = 0

    while seq_len < maximum_length:
        drafts = []
        drafts_probs = []
        x = seq

        # generate gamma drafts
        for i in range(gamma):
            if isinstance(drafter, T5ForConditionalGeneration):
                draft_logits = drafter(input_ids=x, decoder_input_ids=x).logits[..., -1, :].squeeze(0)
            else:
                draft_logits = drafter.generate(x[0])
            draft_logits = norm_logits(draft_logits, temperature, top_k, top_p)
            drafts_probs.append(draft_logits) # q_i(x)
            xi = sample(draft_logits).unsqueeze(0) # x_i ~ q_i(x)
            drafts.append(xi) 
            x = torch.cat((x, xi), dim=1)
            number_of_trials += 1

        # run target model on drafts and get logits of the previous tokens plus one more token
        Mp = target(input_ids=x, decoder_input_ids=x)
        raw_p = Mp.logits[..., seq_len-2:-1, :].squeeze(0) # p_i(x) for the drafts

        # apply normalization
        p = [None] * gamma
        for i in range(gamma):
            p[i] = norm_logits(raw_p[i], temperature, top_k, top_p)[:drafts_probs[0].shape[0]]

        # r1 follows U(0, 1), r2 follows U(0, 1), ..., r_gamma follows U(0, 1)
        r = torch.rand(gamma, device=input_ids.device)
        # n = \min(\left\{i - 1 \, \middle| \, 1 \leq i \leq \gamma, \, r_i > \frac{p_i(x)}{q_i(x)} \right\} \cup \{\gamma\})
        n = min([i for i in range(gamma) if r[i] > p[i][x[0, seq_len + i].item()] / drafts_probs[i][x[0, seq_len + i].item()]] + [gamma])

        total_accept += n

        # adjut the distribution from Mp
        p_p = Mp.logits[..., -1, :].squeeze(0)
        if n < gamma:
            p_p = max_fn(p[n+1] - draft_logits[n+1])

        # sample from the adjusted distribution
        t = sample(p_p).unsqueeze(0)

        # cat seq with n drafts and the sampled token
        generated = torch.cat((*drafts[:n], t), dim=1)
        print(f"Generation step: {n} drafts + 1 token: {generated}")
        seq = torch.cat((seq, generated), dim=1)
        seq_len = seq.shape[1]

    return seq, total_accept / number_of_trials