{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FastLLM.constants import TARGET_MODEL_NAME\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration\n",
    "\n",
    "# load drafter (T5 small) target (TARGET_MODEL_NAME) and tokenizer of TARGET_MODEL_NAME\n",
    "drafter = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "target_model = AutoModelForSeq2SeqLM.from_pretrained(TARGET_MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation step: 0 drafts + 1 token: tensor([[5]])\n",
      "Generation step: 0 drafts + 1 token: tensor([[233]])\n",
      "Generation step: 0 drafts + 1 token: tensor([[233]])\n",
      "Generation step: 0 drafts + 1 token: tensor([[25]])\n",
      "Generation step: 0 drafts + 1 token: tensor([[3]])\n",
      "Generation step: 0 drafts + 1 token: tensor([[5]])\n",
      "Generation step: 0 drafts + 1 token: tensor([[5]])\n",
      "Generation step: 0 drafts + 1 token: tensor([[25]])\n",
      "Generation step: 0 drafts + 1 token: tensor([[3]])\n",
      "Generation step: 1 drafts + 1 token: tensor([[233,   5]])\n",
      "Generation step: 0 drafts + 1 token: tensor([[25]])\n",
      "Generation step: 0 drafts + 1 token: tensor([[25]])\n",
      "Generation step: 1 drafts + 1 token: tensor([[3, 3]])\n",
      "Generation step: 0 drafts + 1 token: tensor([[5]])\n",
      "Generation step: 2 drafts + 1 token: tensor([[  5, 233,  25]])\n",
      "Generation step: 0 drafts + 1 token: tensor([[25]])\n",
      "Generation step: 1 drafts + 1 token: tensor([[3, 3]])\n",
      "Generation step: 1 drafts + 1 token: tensor([[233,   5]])\n",
      "Generation step: 0 drafts + 1 token: tensor([[233]])\n",
      "Hello world....... you.. you.... you you ..... you you .......\n"
     ]
    }
   ],
   "source": [
    "from FastLLM.sampling.speculative_sampling_new import speculative_decoding\n",
    "\n",
    "prefix = \"Hello world \"\n",
    "prefix_ids = tokenizer(prefix, return_tensors=\"pt\").input_ids\n",
    "\n",
    "prefix_ids = prefix_ids.squeeze(0)[:-1]\n",
    "prefix_ids = prefix_ids.unsqueeze(0)\n",
    "\n",
    "test = speculative_decoding(prefix_ids, drafter=drafter, target=target_model, max_len=25, debug=True)\n",
    "text = test[0]\n",
    "decode = tokenizer.decode(text[0])\n",
    "print(decode)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
